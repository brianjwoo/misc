{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import math\n",
    "\n",
    "from numpy.linalg import slogdet, det, solve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.mixture.base import BaseMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.mixture import GaussianMixture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris[:5,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    [1,2],\n",
    "    [4,2],\n",
    "    [1,3],\n",
    "    [4,3],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_rank(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [4, 2],\n",
       "       [1, 3],\n",
       "       [4, 3]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b662401740fc9a0c9690be0d19e48548be0d24ee"
   },
   "source": [
    "# GMM sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "_uuid": "4728e6fe58255084e0e4106a5d5a2977ac6d18ce"
   },
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    \"\"\" Gaussian Mixture Model\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "        k: int , number of gaussian distributions\n",
    "        \n",
    "        seed: int, will be randomly set if None\n",
    "        \n",
    "        max_iter: int, number of iterations to run algorithm, default: 200\n",
    "        \n",
    "    Attributes\n",
    "    -----------\n",
    "       centroids: array, k, number_features\n",
    "       \n",
    "       cluster_labels: label for each data point\n",
    "       \n",
    "    \"\"\"\n",
    "    def __init__(self, C, n_runs):\n",
    "        self.C = C # number of Guassians/clusters\n",
    "        self.n_runs = n_runs\n",
    "        \n",
    "    \n",
    "    def get_params(self):\n",
    "        return (self.mu, self.pi, self.sigma)\n",
    "    \n",
    "    \n",
    "        \n",
    "    def calculate_mean_covariance(self, X, prediction):\n",
    "        \"\"\"Calculate means and covariance of different\n",
    "            clusters from k-means prediction\n",
    "        \n",
    "        Parameters:\n",
    "        ------------\n",
    "        prediction: cluster labels from k-means\n",
    "        \n",
    "        X: N*d numpy array data points \n",
    "        \n",
    "        Returns:\n",
    "        -------------\n",
    "        intial_means: for E-step of EM algorithm\n",
    "        \n",
    "        intial_cov: for E-step of EM algorithm\n",
    "        \n",
    "        \"\"\"\n",
    "        d = X.shape[1]\n",
    "        labels = np.unique(prediction)\n",
    "        self.initial_means = np.zeros((self.C, d))\n",
    "        self.initial_cov = np.zeros((self.C, d, d))\n",
    "        self.initial_pi = np.zeros(self.C)\n",
    "        \n",
    "        counter=0\n",
    "        for label in labels:\n",
    "            ids = np.where(prediction == label) # returns indices\n",
    "            self.initial_pi[counter] = len(ids[0]) / X.shape[0]\n",
    "            self.initial_means[counter,:] = np.mean(X[ids], axis = 0)\n",
    "            de_meaned = X[ids] - self.initial_means[counter,:]\n",
    "            Nk = X[ids].shape[0] # number of data points in current gaussian\n",
    "            self.initial_cov[counter,:, :] = np.dot(self.initial_pi[counter] * de_meaned.T, de_meaned) / Nk\n",
    "            counter+=1\n",
    "        assert np.sum(self.initial_pi) == 1    \n",
    "            \n",
    "        return (self.initial_means, self.initial_cov, self.initial_pi)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _initialise_parameters(self, X):\n",
    "        \"\"\"Implement k-means to find starting\n",
    "            parameter values.\n",
    "            https://datascience.stackexchange.com/questions/11487/how-do-i-obtain-the-weight-and-variance-of-a-k-means-cluster\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "        X: numpy array of data points\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        tuple containing initial means and covariance\n",
    "        \n",
    "        _initial_means: numpy array: (C*d)\n",
    "        \n",
    "        _initial_cov: numpy array: (C,d*d)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        n_clusters = self.C\n",
    "        kmeans = KMeans(n_clusters= n_clusters, init=\"k-means++\", max_iter=500, algorithm = 'auto')\n",
    "        fitted = kmeans.fit(X)\n",
    "        prediction = kmeans.predict(X)\n",
    "        self._initial_means, self._initial_cov, self._initial_pi = self.calculate_mean_covariance(X, prediction)\n",
    "        \n",
    "        \n",
    "        return (self._initial_means, self._initial_cov, self._initial_pi)\n",
    "            \n",
    "        \n",
    "        \n",
    "    def _e_step(self, X, pi, mu, sigma):\n",
    "        \"\"\"Performs E-step on GMM model\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "        X: (N x d), data points, m: no of features\n",
    "        pi: (C), weights of mixture components\n",
    "        mu: (C x d), mixture component means\n",
    "        sigma: (C x d x d), mixture component covariance matrices\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        gamma: (N x C), probabilities of clusters for objects\n",
    "        \"\"\"\n",
    "        N = X.shape[0] \n",
    "        self.gamma = np.zeros((N, self.C))\n",
    "\n",
    "        const_c = np.zeros(self.C)\n",
    "        \n",
    "        \n",
    "        self.mu = self.mu if self._initial_means is None else self._initial_means\n",
    "        self.pi = self.pi if self._initial_pi is None else self._initial_pi\n",
    "        self.sigma = self.sigma if self._initial_cov is None else self._initial_cov\n",
    "\n",
    "        for c in range(self.C):\n",
    "            # Posterior Distribution using Bayes Rule\n",
    "            self.gamma[:,c] = self.pi[c] * mvn.pdf(X, self.mu[c,:], self.sigma[c])\n",
    "\n",
    "        # normalize across columns to make a valid probability\n",
    "        gamma_norm = np.sum(self.gamma, axis=1)[:,np.newaxis]\n",
    "        self.gamma /= gamma_norm\n",
    "\n",
    "        return self.gamma\n",
    "    \n",
    "    \n",
    "    def _m_step(self, X, gamma):\n",
    "        \"\"\"Performs M-step of the GMM\n",
    "        We need to update our priors, our means\n",
    "        and our covariance matrix.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (N x d), data \n",
    "        gamma: (N x C), posterior distribution of lower bound \n",
    "\n",
    "        Returns:\n",
    "        ---------\n",
    "        pi: (C)\n",
    "        mu: (C x d)\n",
    "        sigma: (C x d x d)\n",
    "        \"\"\"\n",
    "        N = X.shape[0] # number of objects\n",
    "        C = self.gamma.shape[1] # number of clusters\n",
    "        d = X.shape[1] # dimension of each object\n",
    "\n",
    "        # responsibilities for each gaussian\n",
    "        self.pi = np.mean(self.gamma, axis = 0)\n",
    "\n",
    "        self.mu = np.dot(self.gamma.T, X) / np.sum(self.gamma, axis = 0)[:,np.newaxis]\n",
    "\n",
    "        for c in range(C):\n",
    "            x = X - self.mu[c, :] # (N x d)\n",
    "            \n",
    "            gamma_diag = np.diag(self.gamma[:,c])\n",
    "            x_mu = np.matrix(x)\n",
    "            gamma_diag = np.matrix(gamma_diag)\n",
    "\n",
    "            sigma_c = x.T * gamma_diag * x\n",
    "            self.sigma[c,:,:]=(sigma_c) / np.sum(self.gamma, axis = 0)[:,np.newaxis][c]\n",
    "\n",
    "        return self.pi, self.mu, self.sigma\n",
    "    \n",
    "    \n",
    "    def _compute_loss_function(self, X, pi, mu, sigma):\n",
    "        \"\"\"Computes lower bound loss function\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (N x d), data \n",
    "        \n",
    "        Returns:\n",
    "        ---------\n",
    "        pi: (C)\n",
    "        mu: (C x d)\n",
    "        sigma: (C x d x d)\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "        C = self.gamma.shape[1]\n",
    "        self.loss = np.zeros((N, C))\n",
    "\n",
    "        for c in range(C):\n",
    "            dist = mvn(self.mu[c], self.sigma[c],allow_singular=True)\n",
    "            self.loss[:,c] = self.gamma[:,c] * (np.log(self.pi[c]+0.00001)+dist.logpdf(X)-np.log(self.gamma[:,c]+0.000001))\n",
    "        self.loss = np.sum(self.loss)\n",
    "        return self.loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Compute the E-step and M-step and\n",
    "            Calculates the lowerbound\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (N x d), data \n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        instance of GMM\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        d = X.shape[1]\n",
    "        self.mu, self.sigma, self.pi =  self._initialise_parameters(X)\n",
    "        \n",
    "        try:\n",
    "            for run in range(self.n_runs):  \n",
    "                self.gamma  = self._e_step(X, self.mu, self.pi, self.sigma)\n",
    "                self.pi, self.mu, self.sigma = self._m_step(X, self.gamma)\n",
    "                loss = self._compute_loss_function(X, self.pi, self.mu, self.sigma)\n",
    "                \n",
    "                if run :\n",
    "                    print(\"PI: \")\n",
    "                    print(self.pi)\n",
    "                    print(\"u: \")\n",
    "                    print(self.mu)\n",
    "                    print(\"sigma: \")\n",
    "                    print(self.sigma)\n",
    "                    print(\"gamma: \")\n",
    "                    print(self.gamma)\n",
    "                    print(\"Iteration: %d Loss: %0.6f\" %(run, loss))\n",
    "\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Returns predicted labels using Bayes Rule to\n",
    "        Calculate the posterior distribution\n",
    "        \n",
    "        Parameters:\n",
    "        -------------\n",
    "        X: ?*d numpy array\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        labels: predicted cluster based on \n",
    "        highest responsibility gamma.\n",
    "        \n",
    "        \"\"\"\n",
    "        labels = np.zeros((X.shape[0], self.C))\n",
    "        \n",
    "        for c in range(self.C):\n",
    "            labels [:,c] = self.pi[c] * mvn.pdf(X, self.mu[c,:], self.sigma[c])\n",
    "        labels  = labels .argmax(1)\n",
    "        return labels \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Returns predicted labels\n",
    "        \n",
    "        Parameters:\n",
    "        -------------\n",
    "        X: N*d numpy array\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        labels: predicted cluster based on \n",
    "        highest responsibility gamma.\n",
    "        \n",
    "        \"\"\"\n",
    "        post_proba = np.zeros((X.shape[0], self.C))\n",
    "        \n",
    "        for c in range(self.C):\n",
    "            # Posterior Distribution using Bayes Rule, try and vectorise\n",
    "            post_proba[:,c] = self.pi[c] * mvn.pdf(X, self.mu[c,:], self.sigma[c])\n",
    "    \n",
    "        return post_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GMM(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-61cb5be4d19d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-127-335af85b0b24>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_runs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_loss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-127-335af85b0b24>\u001b[0m in \u001b[0;36m_e_step\u001b[0;34m(self, X, pi, mu, sigma)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;31m# Posterior Distribution using Bayes Rule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmvn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# normalize across columns to make a valid probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36mpdf\u001b[0;34m(self, x, mean, cov, allow_singular)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_quantiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0mpsd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_PSD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_singular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_pdet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_squeeze_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, M, cond, rcond, lower, check_finite, allow_singular)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_singular\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'singular matrix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0ms_pinv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pinv_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_pinv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: singular matrix"
     ]
    }
   ],
   "source": [
    "gmm.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 2.5],\n",
       "       [4. , 2.5]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.   , 0.   ],\n",
       "        [0.   , 0.125]],\n",
       "\n",
       "       [[0.   , 0.   ],\n",
       "        [0.   , 0.125]]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    \"\"\" Gaussian Mixture Model\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "        k: int , number of gaussian distributions\n",
    "        \n",
    "        seed: int, will be randomly set if None\n",
    "        \n",
    "        max_iter: int, number of iterations to run algorithm, default: 200\n",
    "        \n",
    "    Attributes\n",
    "    -----------\n",
    "       centroids: array, k, number_features\n",
    "       \n",
    "       cluster_labels: label for each data point\n",
    "       \n",
    "    \"\"\"\n",
    "    def __init__(self, C, n_runs, initial_pi=None, initial_mu=None, initial_sigma=None):\n",
    "        self.C = C # number of Guassians/clusters\n",
    "        self.n_runs = n_runs\n",
    "        self._initial_pi = initial_pi\n",
    "        self._initial_means = initial_mu\n",
    "        self._initial_cov = initial_sigma        \n",
    "        self.initial_pi = initial_pi\n",
    "        self.initial_means = initial_mu\n",
    "        self.initial_cov = initial_sigma\n",
    "        \n",
    "    \n",
    "    def get_params(self):\n",
    "        return (self.mu, self.pi, self.sigma)\n",
    "        \n",
    "    def _e_step(self, X, pi, mu, sigma):\n",
    "        \"\"\"Performs E-step on GMM model\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "        X: (N x d), data points, m: no of features\n",
    "        pi: (C), weights of mixture components\n",
    "        mu: (C x d), mixture component means\n",
    "        sigma: (C x d x d), mixture component covariance matrices\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        gamma: (N x C), probabilities of clusters for objects\n",
    "        \"\"\"\n",
    "        N = X.shape[0] \n",
    "        self.gamma = np.zeros((N, self.C))\n",
    "\n",
    "        const_c = np.zeros(self.C)\n",
    "        \n",
    "        \n",
    "        self.mu = self.mu if self._initial_means is None else self._initial_means\n",
    "        self.pi = self.pi if self._initial_pi is None else self._initial_pi\n",
    "        self.sigma = self.sigma if self._initial_cov is None else self._initial_cov\n",
    "\n",
    "        for c in range(self.C):\n",
    "            # Posterior Distribution using Bayes Rule\n",
    "            self.gamma[:,c] = self.pi[c] * mvn.pdf(X, self.mu[c,:], self.sigma[c], allow_singular=True)\n",
    "\n",
    "        # normalize across columns to make a valid probability\n",
    "        gamma_norm = np.sum(self.gamma, axis=1)[:,np.newaxis]\n",
    "        self.gamma /= gamma_norm\n",
    "\n",
    "        return self.gamma\n",
    "    \n",
    "    \n",
    "    def _m_step(self, X, gamma):\n",
    "        \"\"\"Performs M-step of the GMM\n",
    "        We need to update our priors, our means\n",
    "        and our covariance matrix.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (N x d), data \n",
    "        gamma: (N x C), posterior distribution of lower bound \n",
    "\n",
    "        Returns:\n",
    "        ---------\n",
    "        pi: (C)\n",
    "        mu: (C x d)\n",
    "        sigma: (C x d x d)\n",
    "        \"\"\"\n",
    "        N = X.shape[0] # number of objects\n",
    "        C = self.gamma.shape[1] # number of clusters\n",
    "        d = X.shape[1] # dimension of each object\n",
    "\n",
    "        # responsibilities for each gaussian\n",
    "        self.pi = np.mean(self.gamma, axis = 0)\n",
    "\n",
    "        self.mu = np.dot(self.gamma.T, X) / np.sum(self.gamma, axis = 0)[:,np.newaxis]\n",
    "\n",
    "        for c in range(C):\n",
    "            x = X - self.mu[c, :] # (N x d)\n",
    "            \n",
    "            gamma_diag = np.diag(self.gamma[:,c])\n",
    "            x_mu = np.matrix(x)\n",
    "            gamma_diag = np.matrix(gamma_diag)\n",
    "\n",
    "            sigma_c = x.T * gamma_diag * x\n",
    "            self.sigma[c,:,:]=(sigma_c) / np.sum(self.gamma, axis = 0)[:,np.newaxis][c]\n",
    "\n",
    "        return self.pi, self.mu, self.sigma\n",
    "    \n",
    "    \n",
    "    def _compute_loss_function(self, X, pi, mu, sigma):\n",
    "        \"\"\"Computes lower bound loss function\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (N x d), data \n",
    "        \n",
    "        Returns:\n",
    "        ---------\n",
    "        pi: (C)\n",
    "        mu: (C x d)\n",
    "        sigma: (C x d x d)\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "        C = self.gamma.shape[1]\n",
    "        self.loss = np.zeros((N, C))\n",
    "\n",
    "        for c in range(C):\n",
    "            dist = mvn(self.mu[c], self.sigma[c],allow_singular=True)\n",
    "            self.loss[:,c] = self.gamma[:,c] * (np.log(self.pi[c]+0.00001)+dist.logpdf(X)-np.log(self.gamma[:,c]+0.000001))\n",
    "        self.loss = np.sum(self.loss)\n",
    "        return self.loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Compute the E-step and M-step and\n",
    "            Calculates the lowerbound\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (N x d), data \n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        instance of GMM\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        d = X.shape[1]\n",
    "        self.mu = self.initial_means\n",
    "        self.sigma = self.initial_cov\n",
    "        self.pi = self.initial_pi\n",
    "\n",
    "        print(\"PI: \")\n",
    "        print(self.pi)\n",
    "        print(\"u: \")\n",
    "        print(self.mu)\n",
    "        print(\"sigma: \")\n",
    "        print(self.sigma)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            for run in range(self.n_runs):  \n",
    "                self.gamma  = self._e_step(X, self.mu, self.pi, self.sigma)\n",
    "                self.pi, self.mu, self.sigma = self._m_step(X, self.gamma)\n",
    "                loss = self._compute_loss_function(X, self.pi, self.mu, self.sigma)\n",
    "                \n",
    "                if run :\n",
    "                    print(\"PI: \")\n",
    "                    print(self.pi)\n",
    "                    print(\"u: \")\n",
    "                    print(self.mu)\n",
    "                    print(\"sigma: \")\n",
    "                    print(self.sigma)\n",
    "                    print(\"gamma: \")\n",
    "                    print(self.gamma)\n",
    "                    print(\"Iteration: %d Loss: %0.6f\" %(run, loss))\n",
    "\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Returns predicted labels using Bayes Rule to\n",
    "        Calculate the posterior distribution\n",
    "        \n",
    "        Parameters:\n",
    "        -------------\n",
    "        X: ?*d numpy array\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        labels: predicted cluster based on \n",
    "        highest responsibility gamma.\n",
    "        \n",
    "        \"\"\"\n",
    "        labels = np.zeros((X.shape[0], self.C))\n",
    "        \n",
    "        for c in range(self.C):\n",
    "            labels [:,c] = self.pi[c] * mvn.pdf(X, self.mu[c,:], self.sigma[c])\n",
    "        labels  = labels .argmax(1)\n",
    "        return labels \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Returns predicted labels\n",
    "        \n",
    "        Parameters:\n",
    "        -------------\n",
    "        X: N*d numpy array\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        labels: predicted cluster based on \n",
    "        highest responsibility gamma.\n",
    "        \n",
    "        \"\"\"\n",
    "        post_proba = np.zeros((X.shape[0], self.C))\n",
    "        \n",
    "        for c in range(self.C):\n",
    "            # Posterior Distribution using Bayes Rule, try and vectorise\n",
    "            post_proba[:,c] = self.pi[c] * mvn.pdf(X, self.mu[c,:], self.sigma[c])\n",
    "    \n",
    "        return post_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_mu = np.array([[2.1766, 3.7571],[2.3922, 2.9190]])\n",
    "initial_sigma = a#np.array([1.1547, 1.1547])\n",
    "initial_pi = np.array([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((2,2,2))\n",
    "np.fill_diagonal(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GMM(2, 10, initial_mu=initial_mu, initial_sigma=initial_sigma, initial_pi=initial_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [4, 2],\n",
       "       [1, 3],\n",
       "       [4, 3]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PI: \n",
      "[0.5 0.5]\n",
      "u: \n",
      "[[2.1766 3.7571]\n",
      " [2.3922 2.919 ]]\n",
      "sigma: \n",
      "[[[1. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 1.]]]\n",
      "PI: \n",
      "[0.12391729 0.87608271]\n",
      "u: \n",
      "[[2.34359541 2.96005031]\n",
      " [2.52212261 2.4349283 ]]\n",
      "sigma: \n",
      "[[[2.22553761e+00 1.75369697e-03]\n",
      "  [1.75369697e-03 3.83537093e-02]]\n",
      "\n",
      " [[2.24951059e+00 1.13690158e-02]\n",
      "  [1.13690158e-02 2.45765675e-01]]]\n",
      "gamma: \n",
      "[[0.01122303 0.98877697]\n",
      " [0.0085788  0.9914212 ]\n",
      " [0.26245319 0.73754681]\n",
      " [0.21341414 0.78658586]]\n",
      "Iteration: 1 Loss: -9.996341\n",
      "PI: \n",
      "[7.19618774e-04 9.99280381e-01]\n",
      "u: \n",
      "[[2.2450013  3.        ]\n",
      " [2.50018363 2.49963993]]\n",
      "sigma: \n",
      "[[[2.18497566e+00 9.78553103e-18]\n",
      "  [9.78553103e-18 3.19262367e-14]]\n",
      "\n",
      " [[2.24999997e+00 9.18831186e-05]\n",
      "  [9.18831186e-05 2.49999870e-01]]]\n",
      "gamma: \n",
      "[[5.37701924e-17 1.00000000e+00]\n",
      " [3.81286849e-17 1.00000000e+00]\n",
      " [1.68390668e-03 9.98316093e-01]\n",
      " [1.19456842e-03 9.98805432e-01]]\n",
      "Iteration: 2 Loss: -10.200612\n",
      "PI: \n",
      "[0.71100896 0.28899104]\n",
      "u: \n",
      "[[2.44292533 2.38946244]\n",
      " [2.64042167 2.7719572 ]]\n",
      "sigma: \n",
      "[[[ 2.24674248 -0.01683786]\n",
      "  [-0.01683786  0.23778145]]\n",
      "\n",
      " [[ 2.23028176 -0.01228411]\n",
      "  [-0.01228411  0.17603928]]]\n",
      "gamma: \n",
      "[[0.8852675  0.1147325 ]\n",
      " [0.8511232  0.1488768 ]\n",
      " [0.5908579  0.4091421 ]\n",
      " [0.51678725 0.48321275]]\n",
      "Iteration: 3 Loss: -10.177400\n",
      "PI: \n",
      "[0.11022421 0.88977579]\n",
      "u: \n",
      "[[2.37469059 2.93600298]\n",
      " [2.51552316 2.44598854]]\n",
      "sigma: \n",
      "[[[2.23429755e+00 1.74537272e-03]\n",
      "  [1.74537272e-03 5.99014022e-02]]\n",
      "\n",
      " [[2.24975903e+00 7.39035764e-03]\n",
      "  [7.39035764e-03 2.47082763e-01]]]\n",
      "gamma: \n",
      "[[0.01554313 0.98445687]\n",
      " [0.01267295 0.98732705]\n",
      " [0.22332147 0.77667853]\n",
      " [0.1893593  0.8106407 ]]\n",
      "Iteration: 4 Loss: -10.086404\n",
      "PI: \n",
      "[0.00835897 0.99164103]\n",
      "u: \n",
      "[[2.2635131  3.        ]\n",
      " [2.50199345 2.49578529]]\n",
      "sigma: \n",
      "[[[2.19407394e+00 1.42823525e-11]\n",
      "  [1.42823525e-11 4.25048550e-09]]\n",
      "\n",
      " [[2.24999603e+00 1.00512637e-03]\n",
      "  [1.00512637e-03 2.49982236e-01]]]\n",
      "gamma: \n",
      "[[8.24215735e-11 1.00000000e+00]\n",
      " [5.96970783e-11 1.00000000e+00]\n",
      " [1.93536461e-02 9.80646354e-01]\n",
      " [1.40822169e-02 9.85917783e-01]]\n",
      "Iteration: 5 Loss: -9.924796\n",
      "array must not contain infs or NaNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:91: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GMM at 0x7fe2db058390>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.5, 2.5])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 0.5])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(data,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
